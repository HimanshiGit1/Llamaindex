# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ql5WV91VxiCgD8rUhdXy4kSsdPGgBH_7
"""

!pip install python-dotenv

# Ensure llama-index is installed and updated
!pip install --upgrade llama-index

!pip install llama_index.callbacks

from dotenv import load_dotenv
import os

#from llama_index.callbacks import LlamaDebugHandler, CallbackManager

load_dotenv()

# Install the specific LlamaIndex integration for Pinecone vector stores
!pip install llama-index-vector-stores-pinecone

import pinecone
from llama_index.core import VectorStoreIndex, ServiceContext, Settings
from llama_index.vector_stores.pinecone import PineconeVectorStore

from pinecone import Pinecone

!pip install llama-index-llms-ollama

from llama_index.llms.ollama import Ollama

!pip install llama-index-embeddings-huggingface

from llama_index.embeddings.huggingface import HuggingFaceEmbedding

pc = Pinecone(api_key="pcsk_6Av3iw_6qHBCwXBXEaQJHGgZTCCBrq7KAhoE1pvwZWNn5NewW6t3LiR6sQgBH39c6GjYsU")

#Install Colab xterm

!pip install colab-xterm

# Commented out IPython magic to ensure Python compatibility.
# %load_ext colabxterm

# Commented out IPython magic to ensure Python compatibility.
# %xterm

!ollama version

!ollama list

!curl http://127.0.0.1:11434

!ollama pull llama2

!ollama generate "Hello, world!"

if __name__ == "__main__":
    print("RAG...")

    Settings.llm = Ollama(model="llama2", request_timeout=5000)
    Settings.embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-small-en-v1.5")

    index_name = "llamaindex-docs-index"
    pinecone_index = pc.Index(name=index_name)
    vector_store = PineconeVectorStore(pinecone_index=pinecone_index)

    # llama_debug = LlamaDebugHandler(print_trace_on_end=True)
    # callback_manager = CallbackManager(handlers=[llama_debug])
    #service_context = ServiceContext.from_defaults()

    index = VectorStoreIndex.from_vector_store(vector_store=vector_store)

    query = "What is a LlamaIndex?"
    query_engine = index.as_query_engine()
    response = query_engine.query(query)
    print(response)

curl http://localhost:11434